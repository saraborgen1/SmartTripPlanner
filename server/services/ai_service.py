#server/services/ai_service.py

# הקובץ הזה מגדיר את שכבת ה"שירות" של המערכת.
# התפקיד של שכבה זו הוא להיות מתווכת בין שכבת ה־
# API
# (שמגדירה נתיבים ובקשות מהלקוח)
# לבין שכבת ה"סוכן" – המודול שבו מתבצעת התקשורת עם המודל
# AI
# בפועל.
from server.agent import llm_agent

# מגדירים פונקציה בשם
# get_ai_answer
# הפונקציה מקבלת מחרוזת – השאלה של המשתמש.
# הפונקציה מחזירה מחרוזת – התשובה של המודל.
#
# זה בעצם "שכבת תיווך":
# מצד אחד –
# API
# קורא לפונקציה הזאת.
# מצד שני – היא מעבירה את השאלה לפונקציה
# ask_ai
# שנמצאת במודול
# llm_agent
# ושם מתבצע בפועל החיבור לשרת
# Ollama
# והמענה מהמודל.
def get_ai_answer(question: str) -> str:
    # כאן מתבצע הקריאה לפונקציה
    # ask_ai
    # שמוגדרת בקובץ
    # llm_agent.py
    # זו שמבצעת את הבקשה לשרת
    # AI
    # ומחזירה את התשובה.
    return llm_agent.ask_ai(question)
