#מייבא את הספרייה 
# requests
# שמאפשרת לשלוח בקשות 
# HTTP 
# לשרתים – במקרה שלנו לשרת של המודל 
# AI.
import requests

#מגדיר פונקציה בשם 
# ask_ai
# שמקבלת פרמטר 
# question  
# שהוא מחרוזת 
# ומחזירה תשובה גם כמחרוזת
def ask_ai(question: str) -> str:
    try:
        #שולח בקשת 
        # POST
        # לשרת – כלומר, שולח מידע (במקרה הזה שאלה)
        #  למודל 
        # AI 
        # מקומי
        response = requests.post(
            #זו הכתובת  של השרת המקומי שמריץ את המודל 
            # Ollama
            "http://localhost:11434/api/generate",
            #זה המידע שנשלח לגוף הבקשה
            json={
                #המודל שבו נשתמש – במקרה הזה מודל בשם
                # "mistral"
                "model": "mistral",
                #השאלה שנשאלה   
                "prompt": question,
                #אנחנו לא רוצים שהתשובה תגיע בהזרמה (שורה-שורה), אלא במכה אחת
                "stream": False
            },
            #אם השרת לא עונה תוך 30 שניות – נעצור את הבקשה ונעבור לשגיאה
            timeout=30
        )
        #מקבל את התשובה מהשרת וממיר אותה לקובץ 
        # JSON 
        # (כמו מילון בפייתון)
        data = response.json()
        #מחזיר את התשובה מהמודל, אם ישנה. אם לא, מחזיר הודעה מתאימה
        return data.get("response", "לא התקבלה תשובה")
    #אם קרתה שגיאה כלשהי (כמו שהשרת לא עונה או שאין חיבור) – נעבור לפה
    except Exception as e:
        return f"שגיאה בתקשורת עם Ollama: {str(e)}"
